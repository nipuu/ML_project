{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "biXvGbA1ZncC"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import nltk.classify.util\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "import IPython\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "plt.style.use('fivethirtyeight') #538\n",
        "cols = ['id','type','time','author','author_id','re_tweeter','associated_tweet','text','links','hashtags','mentions','reply_count','favorite_count','retweet_count','lang','text_raw'\n",
        "]\n",
        "dataframe = pd.read_csv(\"../input/tweets-bollywood-movie-kabir-singh/twitter_kabir_singh_bollywood_movie.csv\", header=None , names=cols )\n",
        "dataframe.head()\n",
        "dataframe.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKmNRW72ZncF"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import re\n",
        "import seaborn as sns\n",
        "from scipy.stats import chi2\n",
        "from random import seed\n",
        "from random import randrange\n",
        "import random\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from html.parser import HTMLParser\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop2=set(['kabirsingh','arjunreddy','films','album','ww','movies','people' ,'man','diya','see','talk','toh','go','bahut','shahidkapoor','shahid','kabir', 'singh', 'movie', 'bollywood', 'com','twitter', 'much', 'character', 'life', 'like', 'https', 'signalling', 'pic', 'watch', 'one', 'said', 'arjun', 'reddy', 'telugu','mein','hai'])\n",
        "stop3=set(['real','future','film','hindi','sunaa','yaar','apni','congrats','watched','kapoor','watching','society','itfantastic','love','let','day','friends', 'friend', 'ke', 'lots', 'shown','time','hard','review','preeti','story','want','heart','girl','makes','news','advanikiara','public','http'])\n",
        "tweets=[]\n",
        "tweet=[]\n",
        "def strip_non_ascii(string):\n",
        "    ''' Returns the string without non ASCII characters'''\n",
        "    stripped = (c for c in string if 0 < ord(c) < 127)\n",
        "    return ''.join(stripped)\n",
        "stop_words=stop_words|stop2|stop3   \n",
        "emoji_pattern = re.compile(\"[\"\n",
        "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "         u\"\\U00002702-\\U000027B0\"\n",
        "         u\"\\U000024C2-\\U0001F251\"\n",
        "         \"]+\", flags=re.UNICODE)\n",
        "\n",
        "x=[]\n",
        "y=[]\n",
        "import string\n",
        "def clear_punctuation(s):\n",
        "    clear_string = \"\"\n",
        "    for symbol in s:\n",
        "        if symbol not in string.punctuation:\n",
        "            clear_string += symbol\n",
        "    \n",
        "    text=clear_string\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    # remove punctuations\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    long_words=[]\n",
        "    # remove short word\n",
        "    for i in newString.split():\n",
        "        if len(i)>=3 and len(i) <=13:                  \n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()\n",
        "with open('../input/tweets-bollywood-movie-kabir-singh/twitter_kabir_singh_bollywood_movie.csv', 'rt') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    next(reader)\n",
        "    count=0\n",
        "    print(count)\n",
        "    for row in reader:\n",
        "        tweet= dict()\n",
        "        tweet['orig'] = row[15]\n",
        "        tweet['id'] = (float(row[4]))\n",
        "        tweet['pubtime'] = (row[2])\n",
        "        tweet['favcount']=(int(row[12]))\n",
        "        tweet['retweetcount']=(int(row[13]))\n",
        "        # Ignore retweets\n",
        "        if re.match(r'^RT.*', tweet['orig']):\n",
        "            continue\n",
        "        tweet['clean'] = tweet['orig']\n",
        "        tweet['clean'] = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet['clean']))\n",
        "        tweet['clean'] = re.sub(r'[^\\x00-\\x7F]+',' ', tweet['clean'])\n",
        "        tweet['clean'] = re.sub(r':', '', tweet['clean'])\n",
        "        tweet['clean'] = re.sub(r'‚Ä¶', '', tweet['clean'])\n",
        "        tweet['clean'] = emoji_pattern.sub(r'', tweet['clean'])\n",
        "        tweet['clean'] = re.sub(r'\\W', ' ', tweet['clean'])\n",
        "        tweet['clean']= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', tweet['clean'])\n",
        "    # Remove single characters from the start\n",
        "        tweet['clean'] = re.sub(r'\\^[a-zA-Z]\\s+', ' ', tweet['clean']) \n",
        "    # Substituting multiple spaces with single space\n",
        "        tweet['clean'] = re.sub(r'\\s+', ' ', tweet['clean'], flags=re.I)\n",
        "    # Removing prefixed 'b'\n",
        "        tweet['clean'] = re.sub(r'^b\\s+', '', tweet['clean'])\n",
        "        tweet['clean']= \" \".join([i for i in re.sub(r'[^a-zA-Z\\s]', \"\", tweet['clean']).lower().split() if i not in stop_words])\n",
        "        tweet['clean']=clear_punctuation(tweet['clean'])\n",
        "        # Remove all non-ascii characters\n",
        "        tweet['clean'] = strip_non_ascii(tweet['clean'])\n",
        "        # Normalize case\n",
        "        tweet['clean'] = tweet['clean'].lower()\n",
        "        # Remove URLS. \n",
        "        tweet['clean'] = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', tweet['clean'])\n",
        "        ' '.join(word for word in tweet['clean'].split() if len(word)>3)   \n",
        "        blob = TextBlob(tweet['clean'])\n",
        "        tweet['TextBlob']=blob\n",
        "        #x.append(tweet['clean'])  \n",
        "        tweets.append(tweet)\n",
        "        count=count+1\n",
        "        #if(count==9500):\n",
        "          #  break\n",
        "for tweet in tweets:\n",
        "    tweet['polarity'] = float(tweet['TextBlob'].sentiment.polarity)\n",
        "    tweet['subjectivity'] = float(tweet['TextBlob'].sentiment.subjectivity)\n",
        "    if tweet['polarity'] >= 0.1:\n",
        "        #tweet['sentiment'] = 'positive'\n",
        "        tweet['sentiment_val']=2\n",
        "    elif tweet['polarity'] <= -0.1:\n",
        "        #tweet['sentiment'] = 'negative'\n",
        "        tweet['sentiment_val']=3\n",
        "    else:\n",
        "        #tweet['sentiment'] = 'neutral'\n",
        "        tweet['sentiment_val']=1\n",
        "tweets_sorted=tweets\n",
        "tweetdf=pd.DataFrame(tweets_sorted)\n",
        "\n",
        "tweetdf['char_count'] = tweetdf['clean'].apply(len)\n",
        "tweetdf['word_count'] = tweetdf['clean'].apply(lambda x: len(x.split()))\n",
        "tweetdf['word_density'] = tweetdf['char_count'] / (tweetdf['word_count']+1)\n",
        "train2, test2 = train_test_split(tweets_sorted, test_size=0.3, random_state=1)\n",
        "print(tweetdf)\n",
        "traindf=pd.DataFrame(train2)\n",
        "testdf=pd.DataFrame(test2)\n",
        "tweet_s0 = tweetdf[tweetdf.sentiment_val ==3]\n",
        "train_s0 = traindf[traindf.sentiment_val ==3]\n",
        "train_s1=traindf[traindf.sentiment_val ==1]\n",
        "train_s2=traindf[traindf.sentiment_val ==2]\n",
        "test_s0 = testdf[testdf.sentiment_val ==3]\n",
        "test_s1=testdf[testdf.sentiment_val ==1]\n",
        "test_s2=testdf[testdf.sentiment_val ==2]\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import PorterStemmer \n",
        "ps = PorterStemmer()  \n",
        "lemmatizer = WordNetLemmatizer() \n",
        "for index,row in tweet_s0.iterrows():\n",
        "    #print(index,lemmatizer.lemmatize(row['clean']))\n",
        "    row['clean']=lemmatizer.lemmatize(row['clean'])\n",
        "    words=(row['clean']) \n",
        "   \n",
        "    pp = [\" \".join([ps.stem(word) for word in words.split(\" \")]) ]\n",
        "    row['clean']=pp\n",
        "   \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "def mahalanobis(x=None, data=None, cov=None):\n",
        "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data  \n",
        "    x    : vector or matrix of data with, say, p columns.\n",
        "    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n",
        "    cov  : covariance matrix (p x p) of the distribution. If None, will be computed from data.\n",
        "    \"\"\"\n",
        "    x_minus_mu = x - np.mean(data)\n",
        "    if not cov:\n",
        "        cov = np.cov(data.values.T)\n",
        "    inv_covmat = np.linalg.inv(cov)\n",
        "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
        "    mahal = np.dot(left_term, x_minus_mu.T)\n",
        "    return mahal.diagonal()\n",
        "\n",
        "df_x = tweet_s0[['subjectivity', 'polarity','char_count','word_count','word_density']].head(1500)\n",
        "df_x['mahala'] = mahalanobis(x=df_x, data=tweet_s0[['subjectivity', 'polarity','char_count','word_count','word_density']])\n",
        "print(chi2.ppf((1-0.01), df=4))\n",
        "\n",
        "\n",
        "#\n",
        "\n",
        "print(df_x[df_x['mahala'] >13])\n",
        "df_x['hateclass'] = 2\n",
        "df_x.loc[df_x['mahala'] <= 4, 'hateclass'] = 1\n",
        "df_x.loc[df_x['mahala'] >13, 'hateclass'] = 3\n",
        "\n",
        "\n",
        "\n",
        "tweet_s0['mahala']=df_x['mahala']\n",
        "tweet_s0['hateclass']=df_x['hateclass']\n",
        "del tweet_s0['sentiment_val']\n",
        "\n",
        "print(tweet_s0)\n",
        "            \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(tweetdf[['subjectivity', 'polarity','char_count','word_count','word_density']], tweetdf['sentiment_val'],test_size=0.3)\n",
        "\n",
        "\n",
        "def splitDataset(dataset, splitRatio):\n",
        "    trainSize = int(len(dataset) * splitRatio)\n",
        "    \n",
        "    trainSet = []\n",
        "    copy = dataset.values.tolist()\n",
        "    \n",
        "    while len(trainSet) < trainSize:\n",
        "        index = random.randrange(len(copy))\n",
        "        trainSet.append(copy.pop(index))\n",
        "    \n",
        "    return [trainSet, copy]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "trainingSet, testSet = splitDataset(tweet_s0[['subjectivity', 'polarity','char_count','word_count','word_density','mahala','hateclass']], 0.67)\n",
        "#tweet_s0.sort_index(axis=0, level=None, ascending=True, inplace=True, kind='quicksort', na_position='last', sort_remaining=True, by=None)+\n",
        "\n",
        "print(\"WordCloud for Negative words/Hate speech in Negative tweets\")\n",
        "words_cleaned = [word for word in tweet_s0.clean if 'http' not in word and not word.startswith('@') and not word.startswith('#') and word != 'RT']\n",
        "words_without_stopwords = [word for word in words_cleaned if not word in stop_words]\n",
        "all_text = ' '.join(word for word in words_without_stopwords)\n",
        "wordcloud = WordCloud(colormap='Reds', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.show()\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "import math\n",
        "def mean(numbers):\n",
        "    \n",
        "    return sum(numbers)/float(len(numbers))\n",
        " \n",
        "def stdev(numbers):\n",
        "    avg = mean(numbers)\n",
        "    try:\n",
        "        variance = sum([pow(x-avg,2) for x in numbers])/float(len(numbers)-1)\n",
        "        return math.sqrt(variance)\n",
        "    except:\n",
        "        return 0\n",
        "def summarize(dataset):\n",
        "    summaries = [(mean(attribute), stdev(attribute)) for attribute in zip(*dataset)]\n",
        "    del summaries[-1]\n",
        "    return summaries\n",
        "\n",
        "\n",
        "train_x.sort_index(axis=0, level=None, ascending=True, inplace=True, kind='quicksort', na_position='last', sort_remaining=True, by=None)\n",
        "train_y.sort_index(axis=0, level=None, ascending=True, inplace=True, kind='quicksort', na_position='last', sort_remaining=True)\n",
        "\n",
        "\n",
        "for index, row in train_x.iterrows():\n",
        "    #print(index,'\\t',type(row['favcount']),\"\\t\",type(row['retweetcount']))\n",
        "    \n",
        "    row['subjectivity'] = row['subjectivity'].astype(float)\n",
        "    row['polarity'] = row['polarity'].astype(float)\n",
        "\n",
        "    \n",
        "\n",
        "def separateByClass(dataset):\n",
        "    separated = {}\n",
        "    for i in range(len(dataset)):\n",
        "        vector = dataset[i]\n",
        "        if (vector[-1] not in separated):\n",
        "            separated[vector[-1]] = []\n",
        "        separated[vector[-1]].append(vector)\n",
        "    return separated\n",
        "\n",
        "\n",
        "def summarizeByClass(dataset):\n",
        "  \n",
        "   \n",
        "    separated = separateByClass(dataset)\n",
        "    summaries = {}\n",
        "    for classValue, instances in separated.items():\n",
        "        summaries[classValue] = summarize(instances)\n",
        "    return summaries\n",
        "    \n",
        "def calculateProbability(x, mean, stdev):\n",
        "    try:\n",
        "        exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n",
        "    \n",
        "        sol= (1 / (math.sqrt(2*math.pi) * stdev)) * exponent\n",
        "        return sol\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def calculateClassProbabilities(summaries, inputVector):\n",
        "    probabilities = {}\n",
        "    for classValue, classSummaries in summaries.items():\n",
        "        probabilities[classValue] = 1\n",
        "        for i in range(len(classSummaries)):\n",
        "            mean, stdev = classSummaries[i]\n",
        "            x = inputVector[i]\n",
        "            probabilities[classValue] *= calculateProbability(x, mean, stdev)\n",
        "    return probabilities\n",
        "def predict(summaries, inputVector):\n",
        "    probabilities = calculateClassProbabilities(summaries, inputVector)\n",
        "    bestLabel, bestProb = None, -1\n",
        "    for classValue, probability in probabilities.items():\n",
        "        if bestLabel is None or probability > bestProb:\n",
        "            bestProb = probability\n",
        "            bestLabel = classValue\n",
        "    return bestLabel\n",
        "def getAccuracy(testSet, predictions):\n",
        "    correct = 0\n",
        "    for i in range(len(testSet)):\n",
        "        if testSet[i][-1] == predictions[i]:\n",
        "            correct += 1\n",
        "    \n",
        "    sol= (correct/float(len(testSet))) * 100.0\n",
        "    \n",
        "        \n",
        "    return (sol) \n",
        "def getPredictions(summaries, testSet):\n",
        "    predictions = []\n",
        "    for i in range(len(testSet)):\n",
        "        result = predict(summaries, testSet[i])\n",
        "        predictions.append(result)\n",
        "    return predictions\n",
        "\n",
        "summaries = summarizeByClass(trainingSet)\n",
        "\t# test model\n",
        "\n",
        "predictions = getPredictions(summaries, testSet)\n",
        "accuracy = getAccuracy(testSet, predictions)\n",
        "print(\"Accuracy of Naive Bayes:\",accuracy)\n",
        "\n",
        "\n",
        "def predict2(node, row):\n",
        "    if row[node['index']] < node['value']:\n",
        "        if isinstance(node['left'], dict):\n",
        "            return predict2(node['left'], row)\n",
        "        else:\n",
        "            return node['left']\n",
        "    else:\n",
        "        if isinstance(node['right'], dict):\n",
        "            return predict2(node['right'], row)\n",
        "        else:\n",
        "            return node['right']\n",
        "\n",
        "\n",
        "def bagging_predict(trees, row):\n",
        "    predictions = [predict2(tree, row) for tree in trees]\n",
        "    return max(set(predictions), key=predictions.count)\n",
        "def to_terminal(group):\n",
        "    outcomes = [row[-1] for row in group]\n",
        "    return max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, n_features, depth):\n",
        "    left, right = node['groups']\n",
        "    del(node['groups'])\n",
        "    # check for a no split\n",
        "    if not left or not right:\n",
        "        node['left'] = node['right'] = to_terminal(left + right)\n",
        "        return\n",
        "    # check for max depth\n",
        "    if depth >= max_depth:\n",
        "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "        return\n",
        "\t# process left child\n",
        "    if len(left) <= min_size:\n",
        "        node['left'] = to_terminal(left)\n",
        "    else:\n",
        "        node['left'] = get_split(left, n_features)\n",
        "        split(node['left'], max_depth, min_size, n_features, depth+1)\n",
        "    # process right child\n",
        "    if len(right) <= min_size:\n",
        "        node['right'] = to_terminal(right)\n",
        "    else:\n",
        "        node['right'] = get_split(right, n_features)\n",
        "        split(node['right'], max_depth, min_size, n_features, depth+1)\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = dataset.values.tolist()\n",
        "    fold_size = int(len(dataset) / n_folds)\n",
        "    for i in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "def test_split(index, value, dataset):\n",
        "    left, right = list(), list()\n",
        "    for row in dataset:\n",
        "        if row[index] < value:\n",
        "            left.append(row)\n",
        "        else:\n",
        "            right.append(row)\n",
        "    return left, right\n",
        "def gini_index(groups, classes):\n",
        "\t# count all samples at split point\n",
        "    n_instances = float(sum([len(group) for group in groups]))\n",
        "\t# sum weighted Gini index for each group\n",
        "    gini = 0.0\n",
        "    for group in groups:\n",
        "        size = float(len(group))\n",
        "        # avoid divide by zero\n",
        "        if size == 0:\n",
        "            continue\n",
        "        score = 0.0\n",
        "        # score the group based on the score for each class\n",
        "        for class_val in classes:\n",
        "            p = [row[-1] for row in group].count(class_val) / size\n",
        "            score += p * p\n",
        "        # weight the group score by its relative size\n",
        "        gini += (1.0 - score) * (size / n_instances)\n",
        "    return gini\n",
        "\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = list()\n",
        "    for fold in folds:\n",
        "        train_set = list(folds)\n",
        "        train_set.remove(fold)\n",
        "        train_set = sum(train_set, [])\n",
        "        test_set = list()\n",
        "        for row in fold:\n",
        "            row_copy = list(row)\n",
        "            test_set.append(row_copy)\n",
        "            row_copy[-1] = None\n",
        "        predicted = algorithm(train_set, test_set, *args)\n",
        "        actual = [row[-1] for row in fold]\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "        scores.append(accuracy)\n",
        "    return scores\n",
        "def get_split(dataset, n_features):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "    features = list()\n",
        "    while len(features) < n_features:\n",
        "        index = randrange(len(dataset[0])-1)\n",
        "        if index not in features:\n",
        "            features.append(index)\n",
        "    for index in features:\n",
        "        for row in dataset:\n",
        "            groups = test_split(index, row[index], dataset)\n",
        "            gini = gini_index(groups, class_values)\n",
        "            if gini < b_score:\n",
        "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "def test_split(index, value, dataset):\n",
        "    left, right = list(), list()\n",
        "    for row in dataset:\n",
        "        if row[index] < value:\n",
        "            left.append(row)\n",
        "        else:\n",
        "            right.append(row)\n",
        "    return left, right\n",
        "\n",
        "def build_tree(train, max_depth, min_size, n_features):\n",
        "    root = get_split(train, n_features)\n",
        "    split(root, max_depth, min_size, n_features, 1)\n",
        "    return root\n",
        "def subsample(dataset, ratio):\n",
        "    sample = list()\n",
        "    n_sample = round(len(dataset) * ratio)\n",
        "    while len(sample) < n_sample:\n",
        "        index = randrange(len(dataset))\n",
        "        sample.append(dataset[index])\n",
        "    return sample\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
        "    trees = list()\n",
        "    for i in range(n_trees):\n",
        "        sample = subsample(train, sample_size)\n",
        "        tree = build_tree(sample, max_depth, min_size, n_features)\n",
        "        trees.append(tree)\n",
        "    predictions = [bagging_predict(trees, row) for row in test]\n",
        "    return(predictions)\n",
        "\n",
        "dataset = tweet_s0[['subjectivity', 'polarity','char_count','word_count','word_density','mahala','hateclass']]\n",
        "n_folds = 5\n",
        "max_depth = 10\n",
        "min_size = 1\n",
        "sample_size = 1.0\n",
        "n_features = 6\n",
        "\n",
        "for n_trees in [1, 5, 10]:\n",
        "    scores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
        "    print('Trees: %d' % n_trees)\n",
        "    print('Scores: %s' % scores)\n",
        "    print('Mean Accuracy of Random Forest: %.3f%%' % (sum(scores)/float(len(scores))))\n",
        "\n",
        "train_x2, valid_x2, train_y2, valid_y2 = model_selection.train_test_split(tweet_s0[['subjectivity', 'polarity','favcount','retweetcount','char_count','word_count','word_density','mahala']], tweet_s0['hateclass'],test_size=0.33)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "model1 = LogisticRegression()\n",
        "model1 = model1.fit(train_x2, train_y2)\n",
        "pred = model1.predict(valid_x2)\n",
        "print(pred)\n",
        "cm=metrics.confusion_matrix(valid_y2,pred)\n",
        "print(cm)\n",
        "plt.imshow(cm, cmap='binary')\n",
        "\n",
        "\n",
        "#SVM Implementation\n",
        "print(\"SVM\")\n",
        "from sklearn.svm import SVC\n",
        "svclassifier = SVC(kernel='linear')\n",
        "svclassifier.fit(train_x2, train_y2)\n",
        "y_pred = svclassifier.predict(valid_x2)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(valid_y2,y_pred))\n",
        "print(classification_report(valid_y2,y_pred))\n",
        "\n",
        "print(\"NaiveBayes\")\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "y_pred = gnb.fit(train_x2, train_y2).predict(valid_x2)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(valid_y2,y_pred))\n",
        "print(classification_report(valid_y2,y_pred))\n",
        "\n",
        "print(\"Random Forest\")\n",
        "from sklearn import model_selection\n",
        "# random forest model creation\n",
        "rfc = RandomForestClassifier()\n",
        "y_pred = rfc.fit(train_x2, train_y2).predict(valid_x2)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(valid_y2,y_pred))\n",
        "print(classification_report(valid_y2,y_pred))\n",
        "\n",
        "print(\"KNN\")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "y_pred = knn.fit(train_x2, train_y2).predict(valid_x2)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(valid_y2,y_pred))\n",
        "print(classification_report(valid_y2,y_pred))\n",
        "\n",
        "'''\n",
        "print(\"Kernel SVM:\")\n",
        "svclassifier2 = SVC(kernel='poly', degree=8)\n",
        "svclassifier2.fit(train_x2, train_y2)\n",
        "y_pred2 = svclassifier2.predict(valid_x2)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(valid_y2,y_pred2))\n",
        "print(classification_report(valid_y2,y_pred2))\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_10_most_common_words(count_data, count_vectorizer):\n",
        "    import matplotlib.pyplot as plt\n",
        "    words = count_vectorizer.get_feature_names()\n",
        "    total_counts = np.zeros(len(words))\n",
        "    for t in count_data:\n",
        "        total_counts+=t.toarray()[0]\n",
        "    \n",
        "    count_dict = (zip(words, total_counts))\n",
        "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
        "    words = [w[0] for w in count_dict]\n",
        "    counts = [w[1] for w in count_dict]\n",
        "    x_pos = np.arange(len(words)) \n",
        "    \n",
        "    plt.figure(2, figsize=(15, 15/1.6180))\n",
        "    plt.subplot(title='10 most common words')\n",
        "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
        "    sns.barplot(x_pos, counts, palette='husl')\n",
        "    plt.xticks(x_pos, words, rotation=90) \n",
        "    plt.xlabel('words')\n",
        "    plt.ylabel('counts')\n",
        "    plt.show()\n",
        "print(\"CNN\")\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(tweet_s0['clean'], tweet_s0['hateclass'], test_size=0.33, random_state=1000)\n",
        "count_vectorizer = CountVectorizer(stop_words=stop_words)\n",
        "count_data = count_vectorizer.fit_transform(sentences_train)\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(sentences_train)\n",
        "X_train = vectorizer.transform(sentences_train)\n",
        "X_test  = vectorizer.transform(sentences_test)\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "print(\"Accuracy:\", score)\n",
        "\n",
        "\n",
        "pred3 = classifier.predict(X_test)\n",
        "print(confusion_matrix(y_test,pred3))\n",
        "print(classification_report(y_test,pred3))\n",
        "plot_10_most_common_words(count_data, count_vectorizer)\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "input_dim = X_train.shape[1]  # Number of features\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, y_train,epochs=100,verbose=False,validation_data=(X_test, y_test),batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "plot_history(history)\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "maxlen = 70\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#print(text_counts)\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer, CountVectorizer\n",
        "from sklearn import naive_bayes,metrics, linear_model,svm\n",
        "from nltk.corpus import stopwords\n",
        "# Model Generation Using Multinomial Naive Bayes\n",
        "clf = MultinomialNB().fit(X_train, y_train)\n",
        "predicted= clf.predict(X_test)\n",
        "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "tf=TfidfVectorizer()\n",
        "text_tf= tf.fit_transform(tweet_s0.clean)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
        "    text_tf, tweet_s0.hateclass, test_size=0.33, random_state=123)\n",
        "clf = MultinomialNB().fit(X_train2, y_train2)\n",
        "predicted= clf.predict(X_test2)\n",
        "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test2, predicted))\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "import operator\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "vocabulary_size = 8000\n",
        "\n",
        "X_trainnpy = X_train\n",
        "\n",
        "def extractDigits(lst): \n",
        "    return list(map(lambda el:[el], lst)) \n",
        "      \n",
        "              \n",
        "y_trainnpy=np.asarray(extractDigits(y_train.tolist())) \n",
        "\n",
        " \n",
        "class RNNNumpy():\n",
        "    def __init__(self, word_dim, hidden_dim = 100, bptt_truncate = 4):\n",
        "        # assign instance variable\n",
        "        self.word_dim = word_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bptt_truncate = bptt_truncate\n",
        "        # random initiate the parameters\n",
        "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
        "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
        "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    xt = np.exp(x - np.max(x))\n",
        "    return xt / np.sum(xt)\n",
        "\n",
        "def forward_propagation(self, x):\n",
        "    # total num of time steps, len of vector x\n",
        "    T = len(x)\n",
        "    # during forward propagation, save all hidden stages in s, S_t = U .dot x_t + W .dot s_{t-1}\n",
        "    # we also need the initial state of s, which is set to 0\n",
        "    # each time step is saved in one row in s，each row in s is s[t] which corresponding to an rnn internal loop time\n",
        "    s = np.zeros((T+1, self.hidden_dim))\n",
        "    s[-1] = np.zeros(self.hidden_dim)\n",
        "    # output at each time step saved as o, save them for later use\n",
        "    o = np.zeros((T, self.word_dim))\n",
        "    for t in np.arange(T):\n",
        "        # we are indexing U by x[t]. it is the same as multiplying U with a one-hot vector\n",
        "        s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
        "        o[t] = softmax(self.V.dot(s[t]))\n",
        "    return [o, s]\n",
        "\n",
        "RNNNumpy.forward_propagation = forward_propagation\n",
        "\n",
        "\n",
        "\n",
        "def predict(self, x):\n",
        "    o, s = self.forward_propagation(x)\n",
        "    return np.argmax(o, axis = 1)\n",
        "\n",
        "RNNNumpy.predict = predict\n",
        "\n",
        "np.random.seed(10)\n",
        "model = RNNNumpy(vocabulary_size)\n",
        "o, s = model.forward_propagation(X_trainnpy[10])\n",
        "print(o.shape)\n",
        "print(o)\n",
        "\n",
        "predictions = model.predict(X_trainnpy[10])\n",
        "print(predictions.shape)\n",
        "print(predictions) \n",
        "\n",
        "## 2. calculate the loss\n",
        "'''\n",
        "the loss is defined as\n",
        "L(y, o) = -\\frac{1}{N} \\sum_{n \\in N} y_n log(o_n)\n",
        "'''\n",
        "def calculate_total_loss(self, x, y):\n",
        "    L = 0\n",
        "    # for each sentence ...\n",
        "    for i in np.arange(len(y)):\n",
        "        o, s = self.forward_propagation(x[i])\n",
        "        # we only care about our prediction of the \"correct\" words\n",
        "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
        "        # add to the loss based on how off we were\n",
        "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
        "    return L\n",
        "\n",
        "def calculate_loss(self, x, y):\n",
        "    # divide the total loss by the number of training examples\n",
        "    N = np.sum((len(y_i) for y_i in y))\n",
        "    return self.calculate_total_loss(x, y)/N\n",
        "\n",
        "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
        "RNNNumpy.calculate_loss = calculate_loss\n",
        "\n",
        "print(\"Expected Loss for random prediction: %f\" % np.log(vocab_size))\n",
        "print(\"Actual loss: %f\" % model.calculate_loss(X_trainnpy[:1000], y_trainnpy[:1000]))\n",
        "\n",
        "def bptt(self, x, y):\n",
        "    T = len(y)\n",
        "    # perform forward propagation\n",
        "    o, s = self.forward_propagation(x)\n",
        "    # we will accumulate the gradients in these variables\n",
        "    dLdU = np.zeros(self.U.shape)\n",
        "    dLdV = np.zeros(self.V.shape)\n",
        "    dLdW = np.zeros(self.W.shape)\n",
        "    delta_o = o\n",
        "    delta_o[np.arange(len(y)), y] -= 1   # it is y_hat - y\n",
        "    # for each output backwards ...\n",
        "    for t in np.arange(T):\n",
        "        dLdV += np.outer(delta_o[t], s[t].T)    # at time step t, shape is word_dim * hidden_dim\n",
        "        # initial delta calculation\n",
        "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
        "        # backpropagation through time (for at most self.bptt_truncate steps)\n",
        "        # given time step t, go back from time step t, to t-1, t-2, ...\n",
        "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
        "            # print(\"Backprogation step t=%d bptt step=%d\" %(t, bptt_step))\n",
        "            dLdW += np.outer(delta_t, s[bptt_step - 1])\n",
        "            dLdU[:, x[bptt_step]] += delta_t\n",
        "            # update delta for next step\n",
        "            dleta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1]**2)\n",
        "    return [dLdU, dLdV, dLdW]\n",
        "\n",
        "RNNNumpy.bptt = bptt\n",
        "\n",
        "'''\n",
        "verify the gradient by its definition:\n",
        "\\frac{\\partial{L}}{\\partial{\\theta}} = \\lim_{h \\propto 0} \\frac{J(\\theta + h) - J(\\theta - h)}{2h}\n",
        "'''\n",
        "def gradient_check(self, x, y, h = 0.001, error_threshold = 0.01):\n",
        "    # calculate the gradient using backpropagation\n",
        "    bptt_gradients = self.bptt(x, y)\n",
        "    # list of all params we want to check\n",
        "    model_parameters = [\"U\", \"V\", \"W\"]\n",
        "    # gradient check for each parameter\n",
        "    for pidx, pname in enumerate(model_parameters):\n",
        "        # get the actual parameter value from model, e.g. model.W\n",
        "        parameter = operator.attrgetter(pname)(self)\n",
        "        print(\"performing gradient check for parameter %s with size %d. \" %(pname, np.prod(parameter.shape)))\n",
        "        # iterate over each element of the parameter matrix, e.g. (0,0), (0,1)...\n",
        "        it = np.nditer(parameter, flags = ['multi_index'], op_flags=['readwrite'])\n",
        "        while not it.finished:\n",
        "            ix = it.multi_index\n",
        "            # save the original value so we can reset it later\n",
        "            original_value = parameter[ix]\n",
        "            # estimate the gradient using (f(x+h) - f(x-h))/2h\n",
        "            parameter[ix] = original_value + h\n",
        "            gradplus = self.calculate_total_loss([x], [y])\n",
        "            parameter[ix] = original_value - h\n",
        "            gradminus = self.calculate_total_loss([x], [y])\n",
        "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
        "            # reset parameter to the original value\n",
        "            parameter[ix] = original_value\n",
        "            # the gradient for this parameter calculated using backpropagation\n",
        "            backprop_gradient = bptt_gradients[pidx][ix]\n",
        "            # calculate the relative error (|x - y|)/(|x|+|y|)\n",
        "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
        "            # if the error is too large fail the gradient check\n",
        "            if relative_error < error_threshold:\n",
        "                print(\"Gradient check error: parameter = %s ix = %s\" %(pname, ix))\n",
        "                print(\"+h Loss: %f\" % gradplus)\n",
        "                print(\"-h Loss: %f\" % gradminus)\n",
        "                print(\"Estimated gradient: %f\" % estimated_gradient)\n",
        "                print(\"Backpropagation gradient: %f\" % backprop_gradient)\n",
        "                print(\"Relative error: %f\" % relative_error)\n",
        "                return\n",
        "            it.iternext()\n",
        "        print(\"Gradient check for parameter %s passed. \" %(pname))\n",
        "\n",
        "RNNNumpy.gradient_check = gradient_check\n",
        "\n",
        "grad_check_vocab_size = 100\n",
        "np.random.seed(10)\n",
        "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate = 1000)\n",
        "model.gradient_check([0,1,2,3], [1,2,3,4])\n",
        "\n",
        "\n",
        "\n",
        "## 4. SGD implementation\n",
        "'''\n",
        "two step:\n",
        "1. calculate the gradients and perform the updates for one batch\n",
        "2. loop through the training set and adjust the learning rate\n",
        "'''\n",
        "### 4.1. perform one step of SGD\n",
        "def numpy_sgd_step(self, x, y, learning_rate):\n",
        "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
        "    self.U -= learning_rate * dLdU\n",
        "    self.V -= learning_rate * dLdV\n",
        "    self.W -= learning_rate * dLdW\n",
        "RNNNumpy.sgd_step = numpy_sgd_step\n",
        "\n",
        "### 4.2. outer SGD loop\n",
        "'''\n",
        " - model: \n",
        " - X_train:\n",
        " - y_train:\n",
        " - learning_rate:\n",
        " - nepoch:\n",
        " - evaluate loss_after:\n",
        "'''\n",
        "def train_with_sgd(model, X_train, y_train, learning_rate = 0.005, nepoch = 100, evaluate_loss_after = 5):\n",
        "    # keep track of the losses so that we can plot them later\n",
        "    losses = []\n",
        "    num_examples_seen = 0\n",
        "    for epoch in range(nepoch):\n",
        "        # optionally evaluate the loss\n",
        "        if (epoch % evaluate_loss_after == 0):\n",
        "            loss = model.calculate_loss(X_train, y_train)\n",
        "            losses.append((num_examples_seen, loss))\n",
        "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            print(\"%s: loss after num_examples_seen=%d epoch=%d: %f\" %(time, num_examples_seen, epoch, loss))\n",
        "            # adjust the learning rate if loss increases\n",
        "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
        "                learning_rate = learning_rate * 0.5\n",
        "                print(\"setting learning rate to %f\" %(learning_rate))\n",
        "            sys.stdout.flush()\n",
        "        # for each training example...\n",
        "        for i in range(len(y_train)):\n",
        "            # one sgd step\n",
        "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
        "            num_examples_seen += 1\n",
        "\n",
        "np.random.seed(10)\n",
        "model = RNNNumpy(vocab_size)\n",
        "%timeit model.sgd_step(X_trainnpy[10], y_trainnpy[10], 0.005)\n",
        "np.random.seed(10)\n",
        "model = RNNNumpy(vocab_size)\n",
        "losses = train_with_sgd(model, X_trainnpy, y_trainnpy, nepoch = 10, evaluate_loss_after = 1)\n",
        "\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "\n",
        "\n",
        "Encoder = LabelEncoder()\n",
        "Train_Y = Encoder.fit_transform(y_train2)\n",
        "Test_Y = Encoder.fit_transform(y_test2)\n",
        "Naive = naive_bayes.MultinomialNB()\n",
        "Naive.fit(X_train2,Train_Y)\n",
        "# predict the labels on validation dataset\n",
        "predictions_NB = Naive.predict(X_test2)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "\n",
        "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
        "\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(X_train2,Train_Y)\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM = SVM.predict(X_test2)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "#Create KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "\n",
        "#Train the model using the training sets\n",
        "knn.fit(train_x2,train_y2)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = knn.predict(valid_x2)\n",
        "from sklearn import metrics\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy for KNN :\",metrics.accuracy_score(valid_y2, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "8mpWJBAUZnb3",
        "outputId": "09f4e6d7-7c07-4ddd-edf8-f181db4a8189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-eced874c0ead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Any results you write to the current directory are saved as output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input'"
          ]
        }
      ],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "print(os.listdir(\"../input\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzbEfEw4ZncX"
      },
      "outputs": [],
      "source": [
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "tweet_s1 = tweetdf[tweetdf.sentiment_val ==1]\n",
        "tweet_s2 = tweetdf[tweetdf.sentiment_val ==2]\n",
        "\n",
        "#dff = DataFrame(tweet_s0,columns=['subjectivity', 'polarity','favcount','retweetcount','mahala','hateclass'])\n",
        "dff1=DataFrame(tweet_s0,columns=['polarity','subjectivity'])\n",
        "kmeans = KMeans(n_clusters=3).fit(dff1)\n",
        "centroids = kmeans.cluster_centers_\n",
        "print(centroids)\n",
        "f1 = plt.figure()\n",
        "plt.scatter(dff1['polarity'],dff1['subjectivity'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)\n",
        "plt.scatter(centroids[:, 0],centroids[:, 1],  c='red', s=50)\n",
        "\n",
        "f2 = plt.figure()\n",
        "xd=tweet_s0['char_count']\n",
        "yd=tweet_s0['word_count']\n",
        "plt.scatter(xd,yd, s=30, alpha=0.15, marker='o',c='red')\n",
        "\n",
        "# determine best fit line\n",
        "par = np.polyfit(xd,yd, 1, full=True)\n",
        "\n",
        "\n",
        "\n",
        "slope=par[0][0]\n",
        "intercept=par[0][1]\n",
        "xl = [min(xd), max(xd)]\n",
        "yl = [slope*xx + intercept  for xx in xl]\n",
        "\n",
        "# coefficient of determination, plot text\n",
        "variance = np.var(yd)\n",
        "residuals = np.var([(slope*xx + intercept - yy)  for xx,yy in zip(xd,yd)])\n",
        "Rsqr = np.round(1-residuals/variance, decimals=2)\n",
        "plt.text(.9*max(xd)+.1*min(xd),.9*max(yd)+.1*min(yd),'$R^2 = %0.2f$'% Rsqr, fontsize=30)\n",
        "\n",
        "plt.xlabel(\"Character count\")\n",
        "plt.ylabel(\"Word count\")\n",
        "\n",
        "# error bounds\n",
        "yerr = [abs(slope*xx + intercept - yy)  for xx,yy in zip(xd,yd)]\n",
        "par = np.polyfit(xd, yerr, 2, full=True)\n",
        "\n",
        "yerrUpper = [(xx*slope+intercept)+(par[0][0]*xx**2 + par[0][1]*xx + par[0][2]) for xx,yy in zip(xd,yd)]\n",
        "yerrLower = [(xx*slope+intercept)-(par[0][0]*xx**2 + par[0][1]*xx + par[0][2]) for xx,yy in zip(xd,yd)]\n",
        "\n",
        "plt.plot(xl, yl, '-b')\n",
        "#plt.plot(xd, yerrLower, '--r')\n",
        "#plt.plot(xd, yerrUpper, '--r')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\nTOP NEGATIVE TWEETS\")\n",
        "\n",
        "sort_by_polarity = tweet_s0.sort_values('polarity')\n",
        "cleanlist=sort_by_polarity['clean'].tolist()\n",
        "polarlist=sort_by_polarity['polarity'].tolist()\n",
        "subjectlist=sort_by_polarity['subjectivity'].tolist()\n",
        "mahalalist=sort_by_polarity['mahala'].tolist()\n",
        "ct=1\n",
        "for row in cleanlist:\n",
        "    print(ct,row)\n",
        "    ct+=1\n",
        "    if(ct>100):\n",
        "        break\n",
        "\n",
        "pos = len(tweet_s0)\n",
        "neu = len(tweet_s1)\n",
        "neg = len(tweet_s2)\n",
        "labels = 'Positive', 'Neutral', 'Negative'\n",
        "labels2 = 'Mild hate', 'Medium Hate', 'Highly offensive'\n",
        "sizes = [pos, neu, neg]\n",
        "colors = ['yellowgreen', 'gold', 'lightcoral']\n",
        "plt.figure()\n",
        "plt.pie(sizes, labels=labels, colors=colors,\n",
        "        autopct='%1.1f%%', shadow=True, startangle=90)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "print(\"Percentage of positive tweets:\",100*pos/(pos+neg+neu))\n",
        "print(\"Percentage of negative tweets:\",100*neg/(pos+neg+neu))  \n",
        "print(\"Percentage of neutral tweets:\",100*neu/(pos+neg+neu))  \n",
        "print(\"Number of negative tweets:\",neg)\n",
        "\n",
        "maxhate=len((tweet_s0[tweet_s0.hateclass ==3]))\n",
        "midhate=len((tweet_s0[tweet_s0.hateclass ==2]))\n",
        "minhate=len((tweet_s0[tweet_s0.hateclass ==1]))\n",
        "colors = ['olivedrab', 'goldenrod', 'indianred']\n",
        "sizes = [minhate, midhate, maxhate]\n",
        "plt.figure()\n",
        "plt.pie(sizes, labels=labels2, colors=colors,\n",
        "        autopct='%1.1f%%', shadow=True, startangle=90)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "print(\"Percentage of mildly offensive tweets:\",100*minhate/(minhate+midhate+maxhate))\n",
        "print(\"Percentage of moderately hateful tweets:\",100*midhate/(minhate+midhate+maxhate))  \n",
        "print(\"Percentage of highly profane tweets:\",100*maxhate/(minhate+midhate+maxhate))\n",
        "\n",
        "plt.figure()\n",
        "x = [d for d in subjectlist]\n",
        "num_bins = 21\n",
        "n, bins, patches = plt.hist(x, num_bins, normed=1, facecolor='forestgreen', alpha=0.5)\n",
        "plt.xlabel('Subjectivity')\n",
        "plt.ylabel('Probability')\n",
        "plt.title(r'Probability Histogram of subjectivity')\n",
        "\n",
        "plt.figure()\n",
        "x = [d for d in polarlist]\n",
        "num_bins = 21\n",
        "n, bins, patches = plt.hist(x, num_bins, normed=1, facecolor='firebrick', alpha=0.5)\n",
        "plt.xlabel('Polarity')\n",
        "plt.ylabel('Probability')\n",
        "plt.title(r'Probability Histogram of polarity')\n",
        "\n",
        "plt.figure()\n",
        "x = [d for d in sort_by_polarity['word_density'].tolist()]\n",
        "num_bins = 21\n",
        "n, bins, patches = plt.hist(x, num_bins, normed=1, facecolor='orange', alpha=0.5)\n",
        "plt.xlabel('Word density')\n",
        "plt.ylabel('Probability')\n",
        "plt.title(r'Probability Histogram of word density')\n",
        "\n",
        "\n",
        "dff1=DataFrame(tweet_s0,columns=['polarity','subjectivity','word_density','char_count','mahala','word_count'])\n",
        "\n",
        "\n",
        "sns.pairplot(dff1)\n",
        "\n",
        "#print(sort_by_polarity.word_density.cov(sort_by_polarity.subjectivity))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}